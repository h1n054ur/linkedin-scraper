# LinkedIn Selenium Scraper

A powerful, production-ready LinkedIn profile scraper that automates the collection of profile URLs and extracts comprehensive profile data including contact information, posts, comments, profile pictures, and PDFs.

## üöÄ Features

### Smart Cookie Management
- **Automatic Cookie Detection**: Checks for existing authentication cookies
- **One-Time Setup**: Manual login only required once, then fully automated
- **Session Persistence**: Cookies remain valid for weeks/months
- **Anti-Detection**: Advanced browser fingerprinting protection

### Two-Phase Operation

#### Phase 1: URL Collection (`linkedin_url_collector.py`)
- **Smart Search**: Automated LinkedIn search result processing
- **Pagination Support**: Processes multiple search pages (configurable)
- **Duplicate Prevention**: Automatic duplicate URL removal
- **Dictionary Output**: Clean `{url: name}` JSON format
- **Resume Capability**: Can resume from existing URL collections

#### Phase 2: Profile Data Extraction (`linkedin_info_extractor.py`)
- **Contact Information**: Email, phone, websites
- **Profile Pictures**: High-resolution image downloads
- **PDF Export**: LinkedIn profile PDF generation
- **Social Activity**: Posts and comments extraction
- **Comprehensive Metadata**: Profile verification, premium status, etc.

### Data Extraction Capabilities

| Data Type | Description | Format |
|-----------|-------------|---------|
| **Contact Info** | Email, phone, websites | JSON structured |
| **Profile Picture** | High-res profile image | JPG/PNG download |
| **PDF Profile** | LinkedIn's official PDF export | PDF download |
| **Posts** | Recent activity posts | JSON with content |
| **Comments** | User comments on posts | JSON with content |
| **Metadata** | Verification, premium status, connections | JSON structured |

## üõ†Ô∏è Installation & Setup

### Prerequisites
- Python 3.7+
- Chrome browser
- ChromeDriver (place in project root)

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Download ChromeDriver
1. Download from [ChromeDriver](https://chromedriver.chromium.org/)
2. Place `chromedriver` executable in project root
3. Make executable (Linux/Mac): `chmod +x chromedriver`

### 3. Project Structure
```
linkedin-selenium-scraper/
‚îú‚îÄ‚îÄ chromedriver                    # ChromeDriver executable
‚îú‚îÄ‚îÄ linkedin_url_collector.py       # Phase 1: URL collection
‚îú‚îÄ‚îÄ linkedin_info_extractor.py      # Phase 2: Data extraction
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ linkedin_url_collector/         # Generated: cookies & URLs
‚îÇ   ‚îú‚îÄ‚îÄ cookies.json
‚îÇ   ‚îú‚îÄ‚îÄ profile_links.json
‚îÇ   ‚îî‚îÄ‚îÄ search_url_cache.json
‚îî‚îÄ‚îÄ scraped_data/                   # Generated: extracted profiles
    ‚îú‚îÄ‚îÄ 1_John_Smith/
    ‚îÇ   ‚îú‚îÄ‚îÄ John_Smith_info.json
    ‚îÇ   ‚îú‚îÄ‚îÄ profile_picture.jpg
    ‚îÇ   ‚îî‚îÄ‚îÄ profile.pdf
    ‚îî‚îÄ‚îÄ 2_Jane_Doe/
        ‚îú‚îÄ‚îÄ Jane_Doe_info.json
        ‚îú‚îÄ‚îÄ profile_picture.png
        ‚îî‚îÄ‚îÄ profile.pdf
```

## üéØ Usage Guide

### Phase 1: Collect Profile URLs

```bash
python linkedin_url_collector.py
```

**First Run (No Cookies)**:
1. Browser opens ‚Üí Log into LinkedIn manually
2. Navigate to search page
3. Apply filters (location, connections, etc.)
4. Press ENTER when ready
5. Script runs automated collection

**Subsequent Runs (Has Cookies)**:
1. Provide search URL
2. Automated collection starts immediately

**Configuration**:
```python
# In linkedin_url_collector.py
MAX_PAGES = 100  # Number of search pages to process
```

### Phase 2: Extract Profile Data

```bash
python linkedin_info_extractor.py
```

**Automated Process**:
1. Reads URLs from `profile_links.json`
2. Processes each profile individually
3. Extracts all available data
4. Creates organized folders and files
5. Continues until all profiles processed

## üìä Output Formats

### URL Collection Output (`profile_links.json`)
```json
{
  "https://linkedin.com/in/johnsmith": "John Smith",
  "https://linkedin.com/in/janedoe": "Jane Doe",
  "https://linkedin.com/in/bobwilson": "Bob Wilson"
}
```

### Profile Data Output (`{Name}_info.json`)
```json
{
  "profile_info": {
    "name": "John Smith",
    "title": "Software Engineer at Tech Corp",
    "location": "San Francisco, CA",
    "verified": true,
    "premium": false,
    "profile_picture_url": "https://...",
    "extracted_at": "2025-06-22T..."
  },
  "contact_info": {
    "email": "john@example.com",
    "phone": "+1-555-123-4567",
    "websites": [
      {
        "url": "https://johnsmith.dev",
        "display_text": "Personal Website"
      }
    ]
  },
  "activity_summary": {
    "total_posts": 25,
    "total_comments": 15,
    "total_activities": 40
  },
  "posts": [
    {
      "index": 1,
      "type": "original_post",
      "content": "Excited to share my latest project...",
      "extracted_at": "2025-06-22T..."
    }
  ],
  "comments": [
    {
      "index": 1,
      "type": "comment",
      "content": "Great insights! Thanks for sharing...",
      "extracted_at": "2025-06-22T..."
    }
  ],
  "extraction_log": {
    "script_version": "3.0.0-production",
    "extraction_completed_at": "2025-06-22T..."
  }
}
```

## ‚öôÔ∏è Configuration Options

### URL Collector Settings
```python
# Maximum pages to scrape
MAX_PAGES = 100

# Output directory
output_dir = "linkedin_url_collector"
```

### Info Extractor Settings
```python
# Output directory for profile data
output_dir = "scraped_data"

# Scroll settings for posts/comments
max_scrolls = 20

# Download timeouts
timeout = 30
```

## üîí Privacy & Ethics

### Rate Limiting
- Built-in delays between requests
- Random wait times to appear human
- Respectful of LinkedIn's servers

### Data Handling
- Only extracts publicly visible information
- No password or private data access
- Follows LinkedIn's robots.txt guidelines

### Best Practices
- Use for legitimate research purposes
- Respect individuals' privacy
- Don't scrape excessively
- Follow your local data protection laws

## üêõ Troubleshooting

### Common Issues

**ChromeDriver Issues**:
```bash
# Check Chrome version
google-chrome --version

# Download matching ChromeDriver version
# Place in project root with execute permissions
```

**Cookie Problems**:
```bash
# Delete cookies and re-authenticate
rm linkedin_url_collector/cookies.json
python linkedin_url_collector.py
```

**Extraction Failures**:
```bash
# Check browser visibility (for debugging)
# Modify linkedin_info_extractor.py:
options.add_argument('--headless')  # Remove this line
```

**Permission Errors**:
```bash
# Ensure ChromeDriver is executable
chmod +x chromedriver

# Check Python permissions
ls -la chromedriver
```

### Error Codes
- `‚ùå No cookies file found`: Run URL collector first
- `‚ùå Profile links file not found`: No URLs to process
- `‚ùå ChromeDriver not found`: Install ChromeDriver in project root
- `‚ö†Ô∏è Modal did not open`: Contact info may not be available

## üìà Performance Stats

### Speed Benchmarks
- **URL Collection**: ~50-100 profiles/minute
- **Data Extraction**: ~1-2 profiles/minute (comprehensive)
- **Success Rate**: 90-95% for public profiles

### Resource Usage
- **Memory**: ~200-500MB during operation
- **Storage**: ~1-5MB per profile (with images/PDFs)
- **Network**: Minimal bandwidth usage

## üîÑ Advanced Usage

### Batch Processing
```python
# Process specific profiles only
profiles_to_process = [
    "https://linkedin.com/in/specific-profile-1",
    "https://linkedin.com/in/specific-profile-2"
]
```

### Custom Search Filters
```python
# LinkedIn search URL examples:
# Geographic: &geoUrn=%5B"103644278"%5D (US)
# Connections: &network=%5B"S"%2C"O"%5D (2nd+3rd)
# Industry: &industryCompanyVertical=%5B"96"%5D (Software)
```

### Data Analysis Integration
```python
# Load extracted data for analysis
import json
from pathlib import Path

def load_profile_data():
    profiles = []
    for folder in Path("scraped_data").iterdir():
        if folder.is_dir():
            json_file = folder / f"{folder.name.split('_', 1)[1]}_info.json"
            if json_file.exists():
                with open(json_file) as f:
                    profiles.append(json.load(f))
    return profiles
```

## üìú License

This project is provided for educational and research purposes. Users are responsible for complying with LinkedIn's Terms of Service and applicable laws.

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## üìû Support

For issues and questions:
1. Check the troubleshooting section
2. Review LinkedIn's current page structure
3. Update ChromeDriver to latest version
4. Open an issue with detailed error logs

---

**‚ö†Ô∏è Disclaimer**: This tool is for educational purposes. Ensure compliance with LinkedIn's Terms of Service and respect for user privacy.